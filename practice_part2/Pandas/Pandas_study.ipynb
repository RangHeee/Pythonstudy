{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **시리즈 (Series)**\n",
    "\n",
    "- 시리즈(Series) 클래스는 넘파이에서 제공하는 1차원 배열과 비슷하지만, 각 데이터의 의미를 표시하는 인덱스(index)를 붙일 수 있다. 데이터 자체는 값(value)이라고 한다.\n",
    "- 시리즈 = 값(value) + 인덱스(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([9904312, 3448737, 2890451, 2466052],\n",
    "              index=[\"서울\", \"부산\", \"인천\", \"대구\"])\n",
    "s\n",
    "# 인덱스의 길이는 데이터의 길이와 같아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터프레임 (DataFrame)**\n",
    "\n",
    "- 데이터프레임(DataFrame) 클래스는 2차원 행렬 데이터에 인덱스를 붙인 것과 비슷하다.\n",
    "- 하지만 사실은 공통 인덱스를 가지는 열 시리즈(column series)를 딕셔너리로 묶어놓은 것이라고 보는 것이 더 정확하다.\n",
    "- 2차원 배열 데이터는 모든 원소가 같은 자료형을 가져야 하지만, 데이터프레임은 각 열(column)마다 자료형이 다를 수 있기 때문이다.\n",
    "- 2차원이므로 각각의 행 데이터의 이름이 되는 행 인덱스(row index) 뿐 아니라 각각의 열 데이터의 이름이 되는 열 인덱스(column index)도 붙일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터프레임 생성**\n",
    "\n",
    "- 데이터프레임을 만드는 방법은 다양하다. 가장 간단한 방법은 다음과 같다.\n",
    "\n",
    "1. 우선 하나의 열이 되는 데이터를 리스트나 일차원 배열로 준비한다.\n",
    "\n",
    "2. 이 각각의 열에 대한 이름(라벨)을 키로 가지는 딕셔너리를 만든다.\n",
    "\n",
    "3. 이 데이터를 DataFrame 클래스 생성자에 넣는다. 동시에 열방향 인덱스는 columns 인수로, 행방향 인덱스는 index 인수로 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"2015\": [9904312, 3448737, 2890451, 2466052],\n",
    "    \"2010\": [9631482, 3393191, 2632035, 2431774],\n",
    "    \"2005\": [9762546, 3512547, 2517680, 2456016],\n",
    "    \"2000\": [9853972, 3655437, 2466338, 2473990],\n",
    "    \"지역\": [\"수도권\", \"경상권\", \"수도권\", \"경상권\"],\n",
    "    \"2010-2015 증가율\": [0.0283, 0.0163, 0.0982, 0.0141]\n",
    "}\n",
    "columns = [\"지역\", \"2015\", \"2010\", \"2005\", \"2000\", \"2010-2015 증가율\"]\n",
    "index = [\"서울\", \"부산\", \"인천\", \"대구\"]\n",
    "df = pd.DataFrame(data, index=index, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열 데이터의 갱신\n",
    "# \"2010-2015 증가율\"이라는 이름의 열 갱신\n",
    "df[\"2010-2015 증가율\"] = df[\"2010-2015 증가율\"] * 100\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열 데이터의 추가\n",
    "# \"2005-2010 증가율\"이라는 이름의 열 추가\n",
    "df[\"2005-2010 증가율\"] = ((df[\"2010\"] - df[\"2005\"]) / df[\"2005\"] * 100).round(2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열 데이터의 삭제\n",
    "# \"2010-2015 증가율\"이라는 이름의 열 삭제\n",
    "del df[\"2010-2015 증가율\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 데이터 인덱싱\n",
    "# 데이터프레임에서 열 라벨로 시리즈를 인덱싱하면 시리즈가 된다.\n",
    "# 이 시리즈를 다시 행 라벨로 인덱싱하면 개별 데이터가 나온다.\n",
    "df[\"2015\"][\"서울\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터 입출력**\n",
    "\n",
    "- Pandas는 데이터 파일을 읽어 데이터프레임을 만들 수 있다.\n",
    "- 다음처럼 여러가지 포맷을 지원한다.\n",
    "- CSV, Excel, HTML, JSON, HDF5, SAS, STATA, SQL\n",
    "- 이 중에서 CSV 파일 포맷은 데이터 값이 쉽표(comma)로 구분되는 텍스트 파일이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **%%writefile 명령**\n",
    "- 샘플 데이터로 사용할 CSV 파일을 %%writefile 매직(magic) 명령으로 만들어보자.\n",
    "- 이 명령은 셀에 서술한 내용대로 텍스트 파일을 만드는 명령이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sample1.csv\n",
    "c1, c2, c3\n",
    "1, 1.11, one\n",
    "2, 2.22, two\n",
    "3, 3.33, three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CSV 파일 입력**\n",
    "\n",
    "- CSV 파일로부터 데이터를 읽어 데이터프레임을 만들 때는 pandas.read_csv 함수를 사용한다.\n",
    "- 함수의 입력값으로 파일 이름을 넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('sample1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테이블 내의 특정한 열을 행 인덱스로 지정하고 싶으면 index_col 인수를 사용한다.\n",
    "pd.read_csv('sample1.csv', index_col='c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sample3.txt\n",
    "c1        c2        c3        c4\n",
    "0.179181 -1.538472  1.347553  0.43381\n",
    "1.024209  0.087307 -1.281997  0.49265\n",
    "0.417899 -2.002308  0.255245 -1.10515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이가 정해지지 않은 공백이 구분자인 경우에는 \\s+ 정규식(regular expression) 문자열을 사용한다.\n",
    "pd.read_table('sample3.txt', sep='\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sample4.txt\n",
    "파일 제목: sample4.txt\n",
    "데이터 포맷의 설명:\n",
    "c1, c2, c3\n",
    "1, 1.11, one\n",
    "2, 2.22, two\n",
    "3, 3.33, three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자료 파일 중에 건너 뛰어야 할 행이 있으면 skiprows 인수를 사용한다.\n",
    "pd.read_csv('sample4.txt', skiprows=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sample5.csv\n",
    "c1, c2, c3\n",
    "1, 1.11, one\n",
    "2, , two\n",
    "누락, 3.33, three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정한 값을 NaN으로 취급하고 싶으면 na_values 인수에 NaN 값으로 취급할 값을 넣는다.\n",
    "df = pd.read_csv('sample5.csv', na_values=['누락'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CSV 파일 출력**\n",
    "\n",
    "- 지금까지와 반대로 파이썬의 데이터프레임 값을 CSV 파일로 출력하고 싶으면 to_csv 메서드를 사용한다.\n",
    "- 리눅스나 맥에서는 cat 셸 명령으로 파일의 내용을 확인할 수 있다. 윈도우에서는 type 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sample6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!type sample6.csv\n",
    "# 리눅스나 맥에서는 !cat sample6.csv 함수를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index, header 인수를 지정하여 인덱스 및 헤더 출력 여부를 지정하는 것도 가능하다.\n",
    "df.index = [\"a\", \"b\", \"c\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sample9.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!type sample9.csv\n",
    "# 리눅스나 맥에서는 !cat sample9.csv 함수를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **인터넷 상의 CSV 파일 입력**\n",
    "\n",
    "- 웹상에는 다양한 데이터 파일이 CSV 파일 형태로 제공된다.\n",
    "- read_csv 명령 사용시 파일 패스 대신 URL을 지정하면 Pandas가 직접 해당 파일을 다운로드하여 읽어들인다.\n",
    "- 데이터의 수가 많을 경우, 데이터프레임의 표현(representation)은 데이터 앞, 뒤의 일부분만 보여준다.\n",
    "- 보여줄 행의 수는 display.max_rows 옵션으로 정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/datascienceschool/docker_rpython/master/data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 20)  # 앞뒤로 모두 20행만 보여준다.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 앞이나 뒤의 특정 갯수만 보고 싶다면 head 메서드나 tail 메서드를 이용한다.\n",
    "# 메서드 인수로 출력할 행의 수를 넣을 수도 있다.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터프레임 고급 인덱싱**\n",
    "\n",
    "- 데이터프레임에서 특정한 데이터만 골라내는 것을 인덱싱(indexing)이라고 한다.\n",
    "- Pandas는 numpy행렬과 같이 쉼표를 사용한 (행 인덱스, 열 인덱스) 형식의 2차원 인덱싱을 지원하기 위해 다음과 같은 특별한 인덱서(indexer) 속성도 제공한다.\n",
    "- loc : 라벨값 기반의 2차원 인덱싱\n",
    "- iloc : 순서를 나타내는 정수 기반의 2차원 인덱싱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **loc 인덱서**\n",
    "- df.loc[행 인덱싱값]\n",
    "- df.loc[행 인덱싱값, 열 인덱싱값]\n",
    "- 행 인덱싱값은 정수 또는 행 인덱스데이터이고 열 인덱싱값은 라벨 문자열이다.\n",
    "- 인덱스데이터, 인덱스데이터 슬라이스, 인덱스데이터 리스트, 같은 행 인덱스를 가지는 불리언 시리즈 (행 인덱싱의 경우), 또는 위의 값들을 반환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임 예시\n",
    "df = pd.DataFrame(np.arange(10, 22).reshape(3, 4),\n",
    "                  index=[\"a\", \"b\", \"c\"],\n",
    "                  columns=[\"A\", \"B\", \"C\", \"D\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"b\":\"c\"]\n",
    "# loc를 쓰지 않아도 결과는 동일\n",
    "# df.loc[[\"b\", \"c\"]] 리스트 형식으로도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.A > 10, [\"C\", \"D\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **iloc 인덱서**\n",
    "- iloc 인덱서는 loc 인덱서와 반대로 라벨이 아니라 순서를 나타내는 정수(integer) 인덱스만 받는다.\n",
    "- 다른 사항은 loc 인덱서와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2:3, 1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1] = df.iloc[-1] * 2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터프레임의 데이터 조작**\n",
    "- 판다스는 넘파이 2차원 배열에서 가능한 대부분의 데이터 처리가 가능하다.\n",
    "- 추가로 데이터 처리 및 변환을 위한 다양한 함수와 메서드를 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터 갯수 세기**\n",
    "- 가장 간단한 데이터 분석은 데이터의 갯수를 세는 것이다. count 메서드를 사용한다.\n",
    "- 단, NaN 값은 세지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(range(10))\n",
    "s[3] = np.nan\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터프레임에서는 각 열마다 별도로 데이터 갯수를 센다.\n",
    "- 데이터에서 값이 누락된 부분을 찾을 때 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "df = pd.DataFrame(np.random.randint(5, size=(4, 4)), dtype=float)\n",
    "df.iloc[2, 3] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **카테고리 값 세기**\n",
    "- 시리즈의 값이 정수, 문자열, 카테고리 값인 경우에는 value_counts 메서드로 각각의 값이 나온 횟수를 셀 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "s2 = pd.Series(np.random.randint(6, size=100))\n",
    "s2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임에는 value_counts 메서드가 없으므로 각 열마다 별도로 적용해야 한다.\n",
    "df[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **정렬**\n",
    "- 데이터를 정렬하려면 sort_index 메서드 sort_values 메서드를 사용한다.\n",
    "- sort_index 메서드 : 인덱스 값을 기준\n",
    "- sort_values 메서드 : 데이터 값을 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2 시리즈의 각 데이터 값에 따른 데이터 갯수를 인덱스에 따라 정렬하려면 다음처럼 sort_index를 적용\n",
    "s2.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN값이 있는 경우에는 정렬하면 NaN값이 가장 나중으로 간다.\n",
    "s.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 큰 수에서 작은 수로 반대 방향 정렬하려면 ascending=False 인수를 지정\n",
    "s.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임에서 sort_values 메서드를 사용하려면 by 인수로 정렬 기준이 되는 열을 지정해 주어야 한다.\n",
    "df.sort_values(by=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- by 인수에 리스트 값을 넣으면 이 순서대로 정렬 기준의 우선 순위가 된다.\n",
    "- 즉, 리스트의 첫번째 열을 기준으로 정렬한 후 동일한 값이 나오면 그 다음 열로 순서를 따지게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **행/열 합계**\n",
    "- 행과 열의 합계를 구할 때는 sum(axis) 메서드를 사용한다.\n",
    "- axis 인수에는 합계로 인해 없어지는 방향축(0=행, 1=열)을 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df2 = pd.DataFrame(np.random.randint(10, size=(4, 8)))\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행방향 합계를 구할 때는 sum(axis=1) 메서드를 사용\n",
    "df2.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"RowSum\"] = df2.sum(axis=1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열 합계를 구할 때는 sum(axis=0) 메서드를 사용하는데 axis인수의 디폴트 값이 0이므로 axis인수를 생략\n",
    "df2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[\"ColTotal\", :] = df2.sum()\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **apply 변환**\n",
    "- 행이나 열 단위로 더 복잡한 처리를 하고 싶을 때는 apply 메서드를 사용한다.\n",
    "- 인수로 행 또는 열을 받는 함수를 apply 메서드의 인수로 넣으면 각 열(또는 행)을 반복하여 그 함수에 적용시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({\n",
    "    'A': [1, 3, 4, 3, 4],\n",
    "    'B': [2, 3, 1, 2, 3],\n",
    "    'C': [1, 5, 2, 4, 4]\n",
    "})\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 열의 최대값과 최소값의 차이를 구하고 싶으면 다음과 같은 람다 함수를 넣는다.\n",
    "df3.apply(lambda x: x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행에 대해 적용하고 싶으면 axis=1 인수를 쓴다.\n",
    "df3.apply(lambda x: x.max() - x.min(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 열에 대해 어떤 값이 얼마나 사용되었는지 알고 싶다면 value_counts 함수를 넣으면 된다.\n",
    "df3.apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fillna 메서드**\n",
    "- NaN 값은 fillna 메서드를 사용하여 원하는 값으로 바꿀 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.apply(pd.value_counts).fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **astype 메서드**\n",
    "- astype 메서드로 전체 데이터의 자료형을 바꾸는 것도 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.apply(pd.value_counts).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터프레임 인덱스 설정 및 제거**\n",
    "- 데이터프레임에 인덱스로 들어가 있어야 할 데이터가 일반 데이터 열에 들어가 있거나 반대로 일반 데이터 열이어야 할 것이 인덱스로 되어 있을 수 있다.\n",
    "- 이 때는 set_index 명령이나 reset_index 명령으로 인덱스와 일반 데이터 열을 교환할 수 있다.\n",
    "- set_index : 기존의 행 인덱스를 제거하고 데이터 열 중 하나를 인덱스로 설정\n",
    "- reset_index : 기존의 행 인덱스를 제거하고 인덱스를 데이터 열로 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "df1 = pd.DataFrame(np.vstack([list('ABCDE'),\n",
    "                              np.round(np.random.rand(3, 5), 2)]).T,\n",
    "                   columns=[\"C1\", \"C2\", \"C3\", \"C4\"])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_index 메서드로 특정한 열을 인덱스로 설정할 수 있다. 이 때 기존의 인덱스는 없어진다.\n",
    "df2 = df1.set_index(\"C1\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2열을 인덱스로 지정하면 기존의 인덱스는 사라진다.\n",
    "df2.set_index(\"C2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reset_index 메서드를 쓰면 인덱스를 보통의 자료열로 바꿀 수도 있다.\n",
    "- 이 때 인덱스 열은 자료열의 가장 선두로 삽입된다.\n",
    "- 데이터프레임의 인덱스는 정수로 된 디폴트 인덱스로 바뀐다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_index 메서드를 호출할 때 인수 drop=True 로 설정하면 인덱스 열을 보통의 자료열로 올리는 것이 아니라 그냥 버리게 된다.\n",
    "df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터프레임 합성**\n",
    "- 판다스는 두 개 이상의 데이터프레임을 하나로 합치는 데이터 병합(merge)이나 연결(concatenate)을 지원한다.\n",
    "\n",
    "### **merge 함수를 사용한 데이터프레임 병합**\n",
    "- 그 중 merge 함수는 두 데이터 프레임의 공통 열 혹은 인덱스를 기준으로 두 개의 테이블을 합친다.\n",
    "- 이 때 기준이 되는 열, 행의 데이터를 키(key)라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반 데이터 열이 아닌 인덱스를 기준열로 사용하려면 left_index 또는 right_index 인수를 True 로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "    '도시': ['서울', '서울', '서울', '부산', '부산'],\n",
    "    '연도': [2000, 2005, 2010, 2000, 2005],\n",
    "    '인구': [9853972, 9762546, 9631482, 3655437, 3512547]})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    np.arange(12).reshape((6, 2)),\n",
    "    index=[['부산', '부산', '서울', '서울', '서울', '서울'],\n",
    "           [2000, 2005, 2000, 2005, 2010, 2015]],\n",
    "    columns=['데이터1', '데이터2'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, left_on=['도시', '연도'], right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(\n",
    "    [[1., 2.], [3., 4.], [5., 6.]],\n",
    "    index=['a', 'c', 'e'],\n",
    "    columns=['서울', '부산'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    [[7., 8.], [9., 10.], [11., 12.], [13, 14]],\n",
    "    index=['b', 'c', 'd', 'e'],\n",
    "    columns=['대구', '광주'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 명령어 대신 join 메서드를 사용할 수도 있다.\n",
    "df1.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **concat 함수를 사용한 데이터 연결**\n",
    "- concat 함수를 사용하면 기준 열(key column)을 사용하지 않고 단순히 데이터를 연결(concatenate)한다.\n",
    "- 기본적으로는 위/아래로 데이터 행을 연결한다. 단순히 두 시리즈나 데이터프레임을 연결하기 때문에 인덱스 값이 중복될 수 있다.\n",
    "- 옆으로 데이터 열을 연결하고 싶으면 axis=1로 인수를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(\n",
    "    np.arange(6).reshape(3, 2),\n",
    "    index=['a', 'b', 'c'],\n",
    "    columns=['데이터1', '데이터2'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    5 + np.arange(4).reshape(2, 2),\n",
    "    index=['a', 'c'],\n",
    "    columns=['데이터3', '데이터4'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **피봇테이블(pivot table)**\n",
    "- 피봇테이블이란 데이터 열 중에서 두 개의 열을 각각 행 인덱스, 열 인덱스로 사용하여 데이터를 조회하여 펼쳐놓은 것을 말한다.\n",
    "- 판다스는 피봇테이블을 만들기 위한 pivot 메서드를 제공한다.\n",
    "- 첫번째 인수로는 행 인덱스로 사용할 열 이름, 두번째 인수로는 열 인덱스로 사용할 열 이름, 그리고 마지막으로 데이터로 사용할 열 이름을 넣는다.\n",
    "- 판다스는 지정된 두 열을 각각 행 인덱스와 열 인덱스로 바꾼 후 행 인덱스의 라벨 값이 첫번째 키의 값과 같고 열 인덱스의 라벨 값이 두번째 키의 값과 같은 데이터를 찾아서 해당 칸에 넣는다.\n",
    "- 만약 주어진 데이터가 존재하지 않으면 해당 칸에 NaN 값을 넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 데이터는 각 도시의 연도별 인구를 나타낸 것\n",
    "\n",
    "data = {\n",
    "    \"도시\": [\"서울\", \"서울\", \"서울\", \"부산\", \"부산\", \"부산\", \"인천\", \"인천\"],\n",
    "    \"연도\": [\"2015\", \"2010\", \"2005\", \"2015\", \"2010\", \"2005\", \"2015\", \"2010\"],\n",
    "    \"인구\": [9904312, 9631482, 9762546, 3448737, 3393191, 3512547, 2890451, 263203],\n",
    "    \"지역\": [\"수도권\", \"수도권\", \"수도권\", \"경상권\", \"경상권\", \"경상권\", \"수도권\", \"수도권\"]\n",
    "}\n",
    "columns = [\"도시\", \"연도\", \"인구\", \"지역\"]\n",
    "df1 = pd.DataFrame(data, columns=columns)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 데이터를 도시 이름이 열 인덱스가 되고 연도가 행 인덱스가 되어 행과 열 인덱스만 보면 어떤 도시의 어떤 시점의 인구를 쉽게 알 수 있도록 피봇테이블을 아래와 같이 만들 수 있다.\n",
    "- pivot 명령으로 사용하고 행 인덱스 인수로는 \"도시\", 열 인덱스 인수로는 \"연도\", 데이터 이름으로 \"인구\"를 입력하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.pivot(index=\"도시\", columns=\"연도\", values=\"인구\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2005년 인천의 인구는 데이터에 없기 때문에 NaN으로 표시된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기서 행 인덱스와 열 인덱스는 데이터를 찾는 키(key)의 역할을 한다.\n",
    "- 따라서 키 값으로 데이터가 단 하나만 찾아져야 한다.\n",
    "- 만약 행 인덱스와 열 인덱스 조건을 만족하는 데이터가 2개 이상인 경우에는 \"ValueError\"라는 에러가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **그룹분석(group analysis)**\n",
    "- 키가 지정하는 조건에 맞는 데이터가 하나 이상이라서 데이터 그룹을 이루는 경우에는 그룹의 특성을 보여주는 그룹분석을 해야 한다.\n",
    "- 그룹분석은 피봇테이블과 달리 키에 의해서 결정되는 데이터가 여러개가 있을 경우 미리 지정한 연산을 통해 그 그룹 데이터의 대표값을 계산한다.\n",
    "- 판다스에서는 groupby 메서드를 사용하여 다음처럼 그룹분석을 한다.\n",
    "1. 분석하고자 하는 시리즈나 데이터프레임에 groupby 메서드를 호출하여 그룹화를 한다.\n",
    "2. 그룹 객체에 대해 그룹연산을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **groupby 메서드**\n",
    "- groupby 메서드는 데이터를 그룹 별로 분류하는 역할을 한다.\n",
    "- groupby 메서드의 인수로는 열 또는 열의 리스트와 행 인덱스 같은 값을 사용한다.\n",
    "- 연산 결과로 그룹 데이터를 나타내는 GroupBy 클래스 객체를 반환한다.\n",
    "- 이 객체에는 그룹별로 연산을 할 수 있는 그룹연산 메서드가 있다.\n",
    "\n",
    "### **그룹연산 메서드**\n",
    "- groupby 결과, 즉 GroupBy 클래스 객체의 뒤에 붙일 수 있는 그룹연산 메서드는 다양하다.\n",
    "\n",
    "### **자주 사용되는 그룹연산 메서드**\n",
    "1. size, count : 그룹 데이터의 갯수\n",
    "2. mean, median, min, max : 그룹 데이터의 평균, 중앙값, 최소, 최대\n",
    "3. sum, prod, std, var, quantile : 그룹 데이터의 합계, 곱, 표준편차, 분산, 사분위수\n",
    "4. first, last : 그룹 데이터 중 가장 첫번째 데이터와 가장 나중 데이터\n",
    "\n",
    "5. agg, aggregate\n",
    "- 만약 원하는 그룹연산이 없는 경우 함수를 만들고 이 함수를 agg에 전달한다.\n",
    "- 또는 여러가지 그룹연산을 동시에 하고 싶은 경우 함수 이름 문자열의 리스트를 전달한다.\n",
    "6. describe : 하나의 그룹 대표값이 아니라 여러개의 값을 데이터프레임으로 구한다.\n",
    "7. apply : describe 처럼 하나의 대표값이 아닌 데이터프레임을 출력하지만 원하는 그룹연산이 없는 경우에 사용한다.\n",
    "8. transform : 그룹에 대한 대표값을 만드는 것이 아니라 그룹별 계산을 통해 데이터 자체를 변형한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "df2 = pd.DataFrame({\n",
    "    'key1': ['A', 'A', 'B', 'B', 'A'],\n",
    "    'key2': ['one', 'two', 'one', 'two', 'one'],\n",
    "    'data1': [1, 2, 3, 4, 5],\n",
    "    'data2': [10, 20, 30, 40, 50]\n",
    "})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby 명령을 사용하여 그룹 A와 그룹 B로 구분한 그룹 데이터를 만든다.\n",
    "groups = df2.groupby(df2.key1)\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 GroupBy 클래스 객체에는 각 그룹 데이터의 인덱스를 저장한 groups 속성이 있다.\n",
    "groups.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **pivot_table**\n",
    "- Pandas는 pivot 명령과 groupby 명령의 중간 성격을 가지는 pivot_table 명령도 제공한다.\n",
    "- pivot_table 명령은 groupby 명령처럼 그룹분석을 하지만 최종적으로는 pivot 명령처럼 피봇테이블을 만든다.\n",
    "- 즉 groupby 명령의 결과에 unstack을 자동 적용하여 2차원적인 형태로 변형한다. 사용 방법은 다음과 같다.\n",
    "- pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, margins_name='All')\n",
    "   - data: 분석할 데이터프레임 (메서드일 때는 필요하지 않음)\n",
    "   - values: 분석할 데이터프레임에서 분석할 열\n",
    "   - index: 행 인덱스로 들어갈 키 열 또는 키 열의 리스트\n",
    "   - columns: 열 인덱스로 들어갈 키 열 또는 키 열의 리스트\n",
    "   - aggfunc: 분석 메서드\n",
    "   - fill_value: NaN 대체 값\n",
    "   - margins: 모든 데이터를 분석한 결과를 오른쪽과 아래에 붙일지 여부\n",
    "   - margins_name: 마진 열(행)의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 만들었던 피봇테이블은 pivot_table 명령으로 다음과 같이 만들 수도 있다. 인수의 순서에 주의!\n",
    "df1.pivot_table(\"인구\", \"도시\", \"연도\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"margins=True\" 인수를 주면 aggfunc로 주어진 분석 방법을 해당 열의 모든 데이터, 해당 행의 모든 데이터 그리고 전체 데이터에 대해 적용한 결과를 같이 보여준다. aggfunc가 주어지지 않았으면 평균을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.pivot_table(\"인구\", \"도시\", \"연도\", margins=True, margins_name=\"합계\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DatetimeIndex 인덱스**\n",
    "- 시계열 자료는 인덱스가 날짜 혹은 시간인 데이터를 말한다.\n",
    "- 판다스에서 시계열 자료를 생성하려면 인덱스를 DatetimeIndex 자료형으로 만들어야 한다.\n",
    "- DatetimeIndex 인덱스는 다음과 같은 보조 함수를 사용하여 생성한다.\n",
    "   - pd.to_datetime 함수 & pd.date_range 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **pd.to_datetime 함수**\n",
    "- pd.to_datetime 함수를 쓰면 날짜/시간을 나타내는 문자열을 자동으로 datetime 자료형으로 바꾼 후 DatetimeIndex 자료형 인덱스를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = [\"2023, 1, 1\", \"2023, 1, 4\", \"2023, 1, 5\", \"2023, 1, 6\"]\n",
    "idx = pd.to_datetime(date_str)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만들어진 인덱스를 사용하여 시리즈나 데이터프레임을 생성하면 된다.\n",
    "np.random.seed(0)\n",
    "s = pd.Series(np.random.randn(4), index=idx)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **pd.date_range 함수**\n",
    "- pd.date_range 함수를 쓰면 모든 날짜/시간을 일일히 입력할 필요없이 시작일과 종료일 또는 시작일과 기간을 입력하면 범위 내의 인덱스를 생성해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(\"2023-11-1\", \"2023-11-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(start=\"2023-11-1\", periods=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **freq 인수**\n",
    "- freq 인수로 특정한 날짜만 생성되도록 할 수도 있다.\n",
    "1. s: 초\n",
    "2. T: 분\n",
    "3. H: 시간\n",
    "4. D: 일(day)\n",
    "5. B: 주말이 아닌 평일\n",
    "6. W: 주(일요일)\n",
    "7. W-MON: 주(월요일)\n",
    "8. M: 각 달(month)의 마지막 날\n",
    "9. MS: 각 달의 첫날\n",
    "10. BM: 주말이 아닌 평일 중에서 각 달의 마지막 날\n",
    "11. BMS: 주말이 아닌 평일 중에서 각 달의 첫날\n",
    "12. WOM-2THU: 각 달의 두번째 목요일\n",
    "13. Q-JAN: 각 분기의 첫달의 마지막 날\n",
    "14. Q-DEC: 각 분기의 마지막 달의 마지막 날"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(\"2023-1-1\", \"2023-12-31\", freq=\"MS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **shift 연산**\n",
    "- 시계열 데이터의 인덱스는 시간이나 날짜를 나타내기 때문에 날짜 이동 등의 다양한 연산이 가능하다.\n",
    "- 예를 들어 shift 연산을 사용하면 인덱스는 그대로 두고 데이터만 이동할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "ts = pd.Series(np.random.randn(4), index=pd.date_range(\n",
    "    \"2023-1-1\", periods=4, freq=\"M\"))\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.shift(1, freq=\"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **resample 연산**\n",
    "- resample 연산을 쓰면 시간 간격을 재조정하는 리샘플링(resampling)이 가능하다.\n",
    "- 이 때 시간 구간이 작아지면 데이터 양이 증가한다고 해서 업-샘플링(up-sampling)\n",
    "- 시간 구간이 커지면 데이터 양이 감소한다고 해서 다운-샘플링(down-sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **dt 접근자**\n",
    "- datetime 자료형 시리즈에는 dt 접근자가 있어 datetime 자료형이 가진 몇가지 유용한 속성과 메서드를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(pd.date_range(\"2020-12-25\", periods=100, freq=\"D\"))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- year, month, day, weekday 등의 속성을 이용하면 년, 월, 일, 요일 정보를 빼낼 수 있다.\n",
    "- strftime 메서드를 이용하여 문자열을 만드는 것도 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.dt.strftime(\"%Y년 %m월 %d일\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
